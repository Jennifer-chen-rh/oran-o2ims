[
    {
        "alarmDefinitionId": "Watchdog",
        "alarmName": "An alert that should always be firing to certify that Alertmanager is working properly.",
        "alarmDescription": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
    },
    {
        "alarmDefinitionId": "UpdateAvailable",
        "alarmName": "Your upstream update recommendation service recommends you update your cluster.",
        "alarmDescription": "For more information refer to 'oc adm upgrade'"
    },
    {
        "alarmDefinitionId": "ClusterNotUpgradeable",
        "alarmName": "One or more cluster operators have been blocking minor version cluster upgrades for at least an hour.",
        "alarmDescription": "In most cases, you will still be able to apply patch releases. Reason AdminAckRequired.",
        "proposedRepairActions": "For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.<cluster_domain>/settings/cluster/."
    },
    {
        "alarmDefinitionId": "AlertmanagerReceiversNotConfigured",
        "alarmName": "Receivers (notification integrations) are not configured on Alertmanager",
        "alarmDescription": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur.",
        "proposedRepairActions": "Check the OpenShift documentation to learn how to configure notifications with Alertmanager."
    },
    {
        "alarmDefinitionId": "HighOverallControlPlaneMemory",
        "alarmName": "Memory utilization across all control plane nodes is high, and could impact responsiveness and stability.",
        "alarmDescription": "Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond.",
        "proposedRepairActions": "To fix this, increase memory of the control plane nodes."
    },
    {
        "alarmDefinitionId": "NodeClockNotSynchronising",
        "alarmName": "Clock not synchronising.",
        "alarmDescription": "Clock on host is not synchronising. Ensure NTP is configured on this host.",
        "alarmAdditionalFields": {
            "resourceClass": "COMPUTE"
        }
    },
    {
        "alarmDefinitionId": "NodeClockSkewDetected",
        "alarmName": "Clock skew detected.",
        "alarmDescription": "Clock is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.",
        "alarmAdditionalFields": {
            "resourceClass": "COMPUTE"
        }
    },
    {
        "alarmDefinitionId": "IngressWithoutClassName",
        "alarmName": "Ingress without IngressClassName for 1 day",
        "alarmDescription": "This alert fires when there is an Ingress with an unset IngressClassName for longer than one day.",
        "alarmAdditionalFields": {
            "resourceClass": "COMPUTE"
        }
    },
    {
        "alarmDefinitionId": "NodeMemoryHighUtilization",
        "alarmName": "Host is running out of memory.",
        "alarmDescription": "Memory is filling up, has been above memory high utilization threshold for the last 15 minutes",
        "alarmAdditionalFields": {
            "resourceClass": "COMPUTE"
        }
    }
]